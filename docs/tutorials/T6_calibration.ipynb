{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "19fb22f4",
   "metadata": {},
   "source": [
    "# Model Calibration\n",
    "## Why calibrate?\n",
    "Calibration is the process of fine-tuning an agent-based model's input parameters so that its simulated outputs align with real-world data. Without calibration, the model might generate plausible-looking behavior that doesn’t actually reflect reality. Calibration improves the model’s credibility, predictive power, and policy relevance by ensuring it reproduces key observed patterns or outcomes from historical data.\n",
    "\n",
    "## Calibration Process Overview\n",
    "1. Define Calibration Targets:\n",
    "Select measurable real-world data points (e.g., population size, disease prevalence, contraceptive use) that the model should replicate.\n",
    "\n",
    "2. Identify Parameters to Tune:\n",
    "Choose uncertain model parameters that strongly influence the outcomes but lack precise empirical estimates (e.g., agent behavior probabilities, environmental factors, etc.).\n",
    "\n",
    "3. Choose a Calibration Method:\n",
    "Use either:\n",
    "- Manual/heuristic tuning (trial-and-error or expert knowledge), or\n",
    "- Automated optimization (e.g., grid search, random search, genetic algorithms, or Bayesian methods) to systematically explore parameter space.\n",
    "\n",
    "4. Define a Goodness-of-Fit Metric:\n",
    "Quantify how well the model output matches the targets using metrics like root mean squared error (RMSE), likelihood scores, or custom error functions.\n",
    "\n",
    "5. Run the Calibration:\n",
    "Simulate the model repeatedly with different parameter values and evaluate performance using the fit metric.\n",
    "\n",
    "6. Select Best-Fit Parameters:\n",
    "Identify parameter sets that produce model outputs closest to observed data.\n",
    "\n",
    "7. Validate (if possible):\n",
    "Use separate data not involved in calibration to assess the model’s generalizability.\n",
    "\n",
    "In this tutorial, we will walk through the basics of running both a manual and automated calibration. Choosing between manual and automated calibration depends on the model complexity, parameter uncertainty, available data, and computational resources. \n",
    "\n",
    "## Manual Calibration\n",
    "Manual calibration involves adjusting parameters by hand based on expert knowledge or visual inspection of outputs. This can make sense when:\n",
    "- The model has few parameters to tune\n",
    "- You have strong domain knowledge about parameter ranges\n",
    "- You are in the early stages of model development or prototyping\n",
    "- The simulation is computationally expensive or you have limited computational resources\n",
    "- You want to explore model behavior qualitatively\n",
    "\n",
    "The plotting class (in plotting.py) can be used to visually inspect the outputs of common target parameters (e.g. CPR, method mix, TFR, etc.) and compare the model output vs real-world data.\n",
    "\n",
    "## Automated Calibration\n",
    "Automated calibration uses optimization algorithms (like Optuna’s Bayesian optimization) to efficiently search parameter space for the best fit. An automated calibration in FPsim uses the calibration and experiment classes to use Optuna's optimization methods to determine the best free parameters. This makes sense to use when:\n",
    "- Your model has many uncertain parameters (e.g. 5+)\n",
    "- You have access to compute resources to run many simulations (e.g. in parallel on a machine or VM with ample processing power, memory, and storage OR on a cloud computing platform)\n",
    "- You have a large number of target parameters to which you want to calibrate with an unbiased approach\n",
    " \n",
    "A hybrid approach can also be to start with a manual calibration to narrow down plausible ranges and understand model dynamics. Then switch to automated methods to fine-tune parameters and formalize the process. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5e99e47",
   "metadata": {},
   "source": [
    "## Preparing the data\n",
    "\n",
    "In order to run a calibration successfully, we need to ensure that the fpsim/locations directory contains a directory for the country being calibrated (i.e. 'fpsim/locations/kenya'). This directory should also contain:\n",
    "- A model file (i.e. fpsim/locations/kenya/kenya.py)\n",
    "- A data subdirectory with data for the desired calibration targets (see fpsim/locations/README.md for specific files and means of generating each), ideally with the most recently available comprehensive data to compare with the model output \n",
    "\n",
    "Ensure that the data in the aforementioned files are formatted in the same manner as those in `locations/kenya/data`."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e31bbec8",
   "metadata": {},
   "source": [
    "## Running a Manual Calibration\n",
    "### Imports\n",
    "First, we import any needed packages. We also import the plotting class, which is useful in visually inspecting the model vs observed data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "acd4ee2d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import fpsim as fp\n",
    "from fpsim import plotting as plt\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a7f10d67",
   "metadata": {},
   "source": [
    "### Setting Parameters\n",
    "\n",
    "First, we set up our parameters for the simulation(s) used for calibration, including the country name and any specific sim params, such as the population size and start/end year of the sim."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3264f66",
   "metadata": {},
   "outputs": [],
   "source": [
    "country = 'kenya'\n",
    "pars = fp.make_fp_pars(location=country)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c4a3421f0adfd55d",
   "metadata": {},
   "source": [
    "Next we set our free parameters to initial values that we will iteratively tune to optimize the model outputs (to be as close as possible to real-world data). The free parameters below are used for tuning:\n",
    "- fecundity_var_low, fecundity_var_high\n",
    "- exposure_factor\n",
    "- spacing_pref\n",
    "- primary_infertility\n",
    "- age-based exposure (modified in {country}.py)\n",
    "- parity-based exposure (modified in {country}.py)\n",
    "\n",
    "We can also modify the contraceptive choice parameters, which can be useful especially in adjusting the model contraceptive prevalence rate. The `prob_use_year` parameter is helpful to adjust the CPR starting point (seen in the CPR trend plot), and the `prob_use_trend_par` parameter is helpful to adjust the slope in the CPR trend plot. Lastly, the `method_weights` array parameter is useful in tuning the method mix, for example - increasing the % of pill use and decreasing the % of IUD use. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82f46c591a62a32e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initial free parameters for calibration\n",
    "pars['fecundity_var_low'] = 1\n",
    "pars['fecundity_var_high'] = 1\n",
    "pars['exposure_factor'] = 1\n",
    "\n",
    "# Postpartum sexual activity correction or 'birth spacing preference'. Pulls values from {location}/data/birth_spacing_pref.csv by default\n",
    "# Set all to 1 to reset. Option to use 'optimize-space-prefs.py' script in this directory to determine values\n",
    "pars['spacing_pref']['preference'][:3] =  1  # Spacing of 0-6 months\n",
    "pars['spacing_pref']['preference'][3:6] = 1  # Spacing of 9-15 months\n",
    "pars['spacing_pref']['preference'][6:9] = 1  # Spacing of 18-24 months\n",
    "pars['spacing_pref']['preference'][9:] =  1  # Spacing of 27-36 months\n",
    "\n",
    "# Only other simulation free parameters are age-based exposure and parity-based exposure (which you can adjust manually in {country}.py) as well as primary_infertility (set to 0.05 by default)\n",
    "\n",
    "# Adjust contraceptive choice parameters\n",
    "cm_pars = dict(\n",
    "    prob_use_year=2020,  # Time trend intercept\n",
    "    prob_use_trend_par=0.06,   # Time trend parameter\n",
    "    method_weights=np.array([1, 1, 1, 1, 1, 1, 1, 1, 1])  # Weights for the methods in method_list in methods.py (excluding 'none', so starting with 'pill' and ending in 'othmod').\n",
    ")\n",
    "method_choice = fp.SimpleChoice(pars=cm_pars, location=country)     # The contraceptive choice module used (see methods.py for more documentation). We can select RandomChoice, SimpleChoice, or StandardChoice (StandardChoice is selected by default)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f704d358d1a64428",
   "metadata": {},
   "source": [
    "### Running the Simulation\n",
    "\n",
    "We run the simulation with the free parameters specified above:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5fa73144703c03cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run the sim\n",
    "sim = fp.Sim(pars=pars, contraception_module=method_choice)\n",
    "sim.run()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "413fee59",
   "metadata": {},
   "source": [
    "### Plotting the Target Parameters\n",
    "\n",
    "Once the sim run completes, we plot the sim results and the target parameters (comparing the model results vs real-world data). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19344b9cd60b0098",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot sim\n",
    "sim.plot()\n",
    "\n",
    "# Plotting class function which plots the primary calibration targets (method mix, method use, cpr, total fertility rate, birth spacing, age at first birth, and age-specific fertility rate)\n",
    "plt.plot_calib(sim)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "249f112b4f397c3",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "ea0629cbeca920e5",
   "metadata": {},
   "source": [
    "We can see from the plots above that compared to the real-world data in 2020, our model has some adjustments that need to be made. We do this iteratively to see how we can get specific target parameters closer to reality. For example, the model currently results in a TFR that is too low, and the birth space bins have discrepancies (0-12mo is too high, 12-24mo is too high, and 24-48mo is too low). We can modify the free parameters to tune the model to adjust accordingly. Let's try increasing the exposure factor to increase the TFR and modifying the spacing preference factors to account for the current model/data differences:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c92bad23b6318a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initial free parameters for calibration\n",
    "pars['fecundity_var_low'] = 1\n",
    "pars['fecundity_var_high'] = 1\n",
    "pars['exposure_factor'] = 2\n",
    "\n",
    "# Last free parameter, postpartum sexual activity correction or 'birth spacing preference'. Pulls values from {location}/data/birth_spacing_pref.csv by default\n",
    "# Set all to 1 to reset. Option to use 'optimize-space-prefs.py' script in this directory to determine values\n",
    "pars['spacing_pref']['preference'][:4] = .4  # Spacing of 0-12 months\n",
    "pars['spacing_pref']['preference'][4:8] = .2  # Spacing of 12-24 months\n",
    "pars['spacing_pref']['preference'][8:16] = 2  # Spacing of 24-48 months\n",
    "pars['spacing_pref']['preference'][16:] = 1  # Spacing of >48 months"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9eff3f2025f5dc2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Re-run the sim\n",
    "method_choice = fp.SimpleChoice(pars=cm_pars, location=country) \n",
    "sim = fp.Sim(pars=pars, contraception_module=method_choice)\n",
    "sim.run()\n",
    "plt.plot_calib(sim)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a00177b6494dd9f",
   "metadata": {},
   "source": [
    "Our adjustments helped move the model results towards the real-world trends thankfully! As we iteratively modify the free parameters, we can both improve the calibration of the model and simultaneously learn how the parameters affect the model behavior. For example, as increasing the exposure_factor increased the TFR (which is still too low), we can increase it again in hopes of making it closer to the data. \n",
    "\n",
    "There are different strategies of fine-tuning these parameters manually, but one method is to modify the free parameters to get 1-2 target parameters as close as we can before then focusing on a different target parameter to improve via additional free parameter modifications. As these models are quite dynamic, changing one free parameter will often change several target parameters (some more significantly than others); thus, it's worth taking note of which target parameters change and in what direction(s). \n",
    "\n",
    "There is an initial learning curve to understanding which free parameters affect which target parameters, but tuning the model by visual inspection of model vs data in the plots becomes easier and quicker over time.\n",
    "\n",
    "Let's try increasing the exposure_factor once more and also modifying one of the contraceptive choice parameters (`method_weights`) to calibrate the method mix (increasing weights for those with percentages too low and decreasing weights for those with percentages too high):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e030decca2602d2b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initial free parameters for calibration\n",
    "pars['fecundity_var_low'] = 1\n",
    "pars['fecundity_var_high'] = 1\n",
    "pars['exposure_factor'] = 2.5\n",
    "\n",
    "# Last free parameter, postpartum sexual activity correction or 'birth spacing preference'. Pulls values from {location}/data/birth_spacing_pref.csv by default\n",
    "# Set all to 1 to reset. Option to use 'optimize-space-prefs.py' script in this directory to determine values\n",
    "pars['spacing_pref']['preference'][:4] = .4  # Spacing of 0-12 months\n",
    "pars['spacing_pref']['preference'][4:8] = .2  # Spacing of 12-24 months\n",
    "pars['spacing_pref']['preference'][8:16] = 2  # Spacing of 24-48 months\n",
    "pars['spacing_pref']['preference'][16:] = 1  # Spacing of >48 months\n",
    "\n",
    "# Adjust contraceptive choice parameters\n",
    "cm_pars = dict(\n",
    "    prob_use_year=2020,  # Time trend intercept\n",
    "    prob_use_trend_par=0.06,   # Time trend parameter\n",
    "    force_choose=False,        # Whether to force non-users to choose a method ('False' by default)\n",
    "    method_weights=np.array([.5, .5, 1, .7, 1, 1, 1.3, .8, 3])  # Weights for the methods in method_list in methods.py (excluding 'none', so starting with 'pill' and ending in 'othmod').\n",
    ")\n",
    "method_choice = fp.SimpleChoice(pars=cm_pars, location=country)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2accd79baef72c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Re-run the sim\n",
    "sim = fp.Sim(pars=pars, contraception_module=method_choice)\n",
    "sim.run()\n",
    "plt.plot_calib(sim)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "99e0dfbcbbd3cc52",
   "metadata": {},
   "source": [
    "Both of these changes helped us improve these target parameters! The TFR (and ASFR) both shifted closer to the data, and the method mix looks much closer as well. As was mentioned however, changing free parameters doesn't necessarily adjust target parameters 1-1 (they rarely do!); notice how these changes resulted in the birth space bin of 12-24mo jumping back up when we had tuned it to be lower (and closer to the data) previously. The good news is that we already learned how to adjust the birth space bins by modifying the spacing_pref param; thus, we can keep the free parameters we've adjusted so far and decrease the spacing_pref 12-24mo weight again to continue the calibration. \n",
    "\n",
    "We continue in this manner, iteratively improving the target parameters to which we are calibrating. We can either calibrate solely in this manner (if limited in computational resources and/or in the early stages of model development), or we can perform an approximate calibration and then use the chosen free parameters to help narrow the parameter ranges we want to sweep in an automatic calibration."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd2a91b1e1f93cf2",
   "metadata": {},
   "source": [
    "## Running an Automated Calibration\n",
    "### Setting the Parameters\n",
    "\n",
    "If running an automated calibration, we do the same step of defining the simulation parameters. We then define a dictionary containing parameters we want to sweep in the calibration in one of the two following formats:\n",
    "        \n",
    "        Either a dict of arrays or lists in order best-low-high, e.g.::\n",
    "\n",
    "            calib_pars = dict(\n",
    "                exposure_factor    = [1.0, 0.5,  1.5],\n",
    "                fecundity_var_high = [1,   0.75, 3.0],\n",
    "            )\n",
    "\n",
    "        Or the same thing, as a dict of dicts::\n",
    "\n",
    "            calib_pars = dict(\n",
    "                exposure_factor    = dict(best=1.0, low=0.5,  high=1.5),\n",
    "                fecundity_var_high = dict(best=1,   low=0.75, high=3.0),\n",
    "            )\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43328afab83bdde0",
   "metadata": {},
   "outputs": [],
   "source": [
    "pars = dict(location=country)\n",
    "pars['n_agents'] = 1000  # Population size; set very small here only for the purpose of runtime\n",
    "pars['end_year'] = 2020  # 1960 - 2020 is the normal date range\n",
    "\n",
    "# Free parameters for calibration\n",
    "freepars = dict(\n",
    "        fecundity_var_low = [0.95, 0.925, 0.975],       # [best, low, high]\n",
    "        fecundity_var_high = [1.05, 1.0, 1.3],\n",
    "        exposure_factor = [2.0, 0.95, 2.5],\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42942ce86885bb4c",
   "metadata": {},
   "source": [
    "When setting up calibration parameters for a model, the 'best', 'low', and 'high' values define the starting point and the allowable search space for optimization. The 'best' value represents your current best guess or default setting based on prior knowledge, literature, or previous model use. It is not used to guide the optimization algorithm directly but serves as a baseline for comparison to assess how much the calibration improves model fit. The 'low' and 'high' values define the range of plausible values that Optuna will explore during calibration. These should be wide enough to allow the optimizer to find better-fitting parameter combinations, but constrained enough to reflect realistically plausible bounds. Thoughtful selection of these values ensures both meaningful calibration results and computational efficiency.\n",
    "\n",
    "Once we set up our parameters and free parameters, we can define and run the calibration. We can optionally provide a specific number of trials (`n_trials`) used as well. This indicates the number of trials, i.e. sim runs, per worker. The number of workers (by default per the Calibration class, is the number of CPUs). For the sake of the runtime for this tutorial, we will specify a low number of trials here. Other optional parameters can be found in the documentation in the Calibration class (`fpsim/calibration.py`)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8761c8441444049e",
   "metadata": {},
   "source": [
    "### Running the Calibration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4fc6edd6d7f15543",
   "metadata": {},
   "outputs": [],
   "source": [
    "calibration = fp.Calibration(pars, calib_pars=freepars, n_trials=2)\n",
    "calibration.calibrate()\n",
    "calibration.summarize()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b0560d6ab9f13ce",
   "metadata": {},
   "source": [
    "### Analyzing the Results\n",
    "\n",
    "We can see that the calibration provided us with the optimal set of free parameters as well as indicated the reduction in mismatch (and percentage improvement) after the calibration was run. The mismatch is calculated using the default_flags that are set to `1` in the Experiment class (`experiment.py`), some of which can be disabled if desired by setting to `0`. Additionally, we can analyze the calibration results by using some of the calibration class plotting functions: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6efe741f43fa03e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "The plot_trend function below visualizes the evolution of the mismatch metric over trials during the calibration and highlights the progression of the best (lowest) mismatch values. It's useful to see whether the calibration is converging or plateauing and identify how quickly the algorithm found good parameter regions. \n",
    "'''\n",
    "\n",
    "fig = calibration.plot_trend()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e353afc875efca5c",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "The plot_best function below visualizes only the parameter sets from trials that performed best, using pairwise plots colored by mismatch. The pairwise plot matrix shows scatterplots colored by mismatch in the lower triangle, histograms of parameter distributions on the diagonal, and KDE contours in the upper triangle. These can help identify correlations between parameters that lead to good performance. It's useful for exploring the parameter space that led to good calibration fits.\n",
    "'''\n",
    "\n",
    "fig = calibration.plot_best()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42ec26ec2f7f51f1",
   "metadata": {},
   "source": [
    "We can then continue to adjust the free parameter ranges, iteratively setting the 'Best parameter values' as the next 'best' params in the free parameter dictionary.\n",
    "\n",
    "### When is a Calibration Sufficient?\n",
    "Deciding when a calibration is sufficient is part science, part judgment. While there is no universal threshold, you can use a combination of quantitative indicators and practical considerations to determine sufficiency. A key metric to monitor is the mismatch — the numerical value representing the error between model output and observed data. Calibration is generally considered sufficient when the best mismatch has stabilized over a series of trials, indicating that further optimization yields diminishing returns.\n",
    "\n",
    "You can visualize this trend using plot_trend(), which shows how the mismatch improves over time. A flattening curve suggests that the optimizer has converged. Additionally, plot_best() helps examine the distribution of parameters from the best-performing trials. A concentrated cluster of low-mismatch solutions is a good sign of a stable and well-behaved calibration.\n",
    "\n",
    "It’s also important to assess parameter realism. Calibrated values should fall well within plausible ranges and not sit at the bounds of their allowed values — this may indicate a need to expand parameter ranges or run more trials. The quality of fit across all outputs should be visually and statistically evaluated using post-calibration plots. A good calibration improves fit across multiple targets without overfitting any single metric.\n",
    "\n",
    "Lastly, reproducibility is key: rerunning the calibration should produce similar results. If the results vary significantly between runs/with different seeds, consider increasing the number of trials or reviewing your parameter bounds. Calibration is complete not just when the mismatch is low, but when the parameter values are stable, plausible, and generate outputs that align well with observed data.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "395da2c8e7bb5f37",
   "metadata": {},
   "source": [
    "## Using the Optimized Free Parameters\n",
    "\n",
    "Once we calibrate a model and determine the optimal set of free parameters, we can use them for a model either by setting them when defining the simulation 'pars' dictionary in an analysis script (as we did earlier in this tutorial), or if we want to continually use them we can set them in the `scalar_pars` function in the `{model}.py` file (i.e. 'kenya.py') like so:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "551edc968f7708ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "def scalar_pars():\n",
    "    scalar_pars = {\n",
    "        'location':             'kenya',\n",
    "        'postpartum_dur':       23,\n",
    "        'exposure_factor':        1.5,\n",
    "    }\n",
    "    return scalar_pars"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
